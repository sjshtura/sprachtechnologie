{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/* Style des gesamten Dokuments */\n",
       "#notebook-container {\n",
       "\tfont-family: \"NimbusMonL-ReguObli\";\n",
       "\tfont-size: 120%\n",
       "}\n",
       "\n",
       "/* Style für die Überschrift: Zentriert diese und stellt sie fett dar. */\n",
       ".headline {\n",
       "\ttext-align: center;\n",
       "\tfont-weight: bold;\n",
       "\tfont-size: 185.7%\n",
       "}\n",
       "\n",
       "/* Style für die Aufgabenbeschreibung. Z.B.: \"Übung zum Thema...\" */\n",
       ".description {\n",
       "\ttext-align: center;\n",
       "\tfont-size: 145.7%\n",
       "}\n",
       "\n",
       "/* Hebt das Abgabedatum fett und kursiv hervor */\n",
       "#submission {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "\n",
       "/* Style für das eigentliche Thema. Z.B.: \"Intelligenz\" */\n",
       "#topic {\n",
       "\tfont-style: italic;\n",
       "}\n",
       "\n",
       ".task_description {\n",
       "\tmargin-bottom: 20px;\n",
       "}\n",
       "\n",
       "/* Hebt die Aufgabennummerierung fett hervor. */\n",
       ".task {\n",
       "\tfont-style: normal;\n",
       "\tfont-weight: bold;\n",
       "\tfont-size: 120%;\n",
       "\tborder-bottom: 2px solid black;\n",
       "  background-color: #97CAEF;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 50px;\n",
       "  padding-right: 50px;\n",
       "}\n",
       "\n",
       ".subtask {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #CAFAFE;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 25px;\n",
       "  padding-right: 25px;\n",
       "}\n",
       "\n",
       ".l1 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #14A76C;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".l2 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #FFE400;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".l3 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #FF652F;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".points {\n",
       "\tfont-style: italic;\n",
       "}\n",
       "\n",
       "ol.lower_roman {\n",
       "    list-style-type: lower-roman;\n",
       "}\n",
       "\n",
       "ol.characters {\n",
       "    list-style-type: lower-alpha;\n",
       "}\n",
       "\n",
       "/* Style einer Code-Cell */\n",
       ".CodeMirror-code {\n",
       "\tbackground-color: #ededed\n",
       "}\n",
       "\n",
       "/* Style eines Kommentars im Code ändern. */\n",
       ".cm-s-ipython span.cm-comment {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-atom {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-number {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Keywords ändern */\n",
       ".cm-s-ipython span.cm-keyword {\n",
       "\tcolor: #B000B0\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-def {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style einer Python-Variable ändern */\n",
       ".cm-s-ipython span.cm-variable {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style einer Property ändern */\n",
       ".cm-s-ipython span.cm-property {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Operators ändern */\n",
       ".cm-s-ipython span.cm-operator {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Strings ändern */\n",
       ".cm-s-ipython span.cm-string {\n",
       "\tcolor: brown;\n",
       "}\n",
       "\n",
       "/* Style einer eingebauten Funktion ändern (z.B. \"open\") */\n",
       ".cm-s-ipython span.cm-builtin {\n",
       "\n",
       "}\n",
       "\n",
       "/* Hebt hervor, welche Klammern zueinander passen */\n",
       ".cm-s-ipython .CodeMirror-matchingbracket {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-variable-2 {\n",
       "\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>\" + open(\"style.css\").read() + \"</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"headline\">\n",
    "Language Technology / Sprachtechnologie\n",
    "<br><br>\n",
    "Wintersemester 2019/2020\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"description\">\n",
    "    Übung zum Thema <i id=\"topic\">\"Sentiment Analysis\"</i>\n",
    "    <br><br>\n",
    "    Deadline Abgabe: <i #id=\"submission\">30.01.2020</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os.path\n",
    "from nltk.corpus import stopwords\n",
    "DATASET_URL = 'https://drive.google.com/uc?export=download&id=1ME5oahBFWW18LzPTB_tgMDExB3qK0GGT'\n",
    "DATASET_PATH = 'amazon_reviews_sentiments.tsv'\n",
    "\n",
    "if not os.path.isfile(DATASET_PATH):\n",
    "    wget.download(DATASET_URL, DATASET_PATH)\n",
    "df = pd.read_csv(DATASET_PATH, names=['Statement', 'label'], sep='\\t')\n",
    "\n",
    "statements = df[\"Statement\"]\n",
    "labels = df[\"label\"]\n",
    "\n",
    "train_statements = statements[:-200]\n",
    "train_labels = labels[:-200]\n",
    "test_statements = statements[-200:]\n",
    "test_labels = labels[-200:]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Präsenzübung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with tf-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 11.5:</i> <br>\n",
    "</div>\n",
    "\n",
    "Train the naive bayes classifier using the TF_IDF feature set of the train data. <br>\n",
    "Then test the classifier's performance on the test set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.5.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Explain the following code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2500, stop_words=stopwords.words('english'), lowercase=True)  \n",
    "features_train = vectorizer.fit_transform(train_statements).toarray()\n",
    "features_test = vectorizer.transform(test_statements).toarray() \n",
    "print(features_train)\n",
    "\n",
    "X_train = features_train\n",
    "y_train = train_labels\n",
    "X_test = features_test\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "Creating tf-idf features with maximum features of 2500, nltk english stopwords with lowercasing for train and test statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.5.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Apply the naive bayes algorithm and train the classifier using the train set using the 'fit' method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "# TODO train the classifier on the train set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# fast better classifier\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try naive bayes algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.5.3</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Evaluate the classifier on the test set using confusion metric, accuracy and classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "\n",
    "cf = confusion_matrix(y_test,predictions)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cf, cmap=\"GnBu\", annot=True, fmt='g');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.5.4</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Find three statements that the classifier fails. Please run the following code to test the classifier. Give a justification for the classifier failure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process user input. Processing continues until the user says goodbye. \n",
    "text = \"\"\n",
    "while text != \"goodbye\":\n",
    "    # Read user input\n",
    "    text = input()\n",
    "    test_features = vectorizer.transform([text]).toarray()  \n",
    "    print('Positive Sentiment' if clf.predict(test_features) == 1 else 'Negative sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i am sad –– *sad* has a low tf-idf score for negative sentiment text\n",
    "\n",
    "product is shit –– *shit* is unknown to classifier\n",
    "\n",
    "i am not satisfied with the product –– *not* is is not considered as removed in stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.5.5</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "The following code snippet generates false positives and false negatives. Please observe the classifier's first ten wrong judgements and describe them briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkout false positive examples\n",
    "\n",
    "no_example = 10\n",
    "\n",
    "\n",
    "for i, sample in enumerate(X_test):\n",
    "    if y_test.values[i] != int(clf.predict([sample])[0]):\n",
    "        print(\"Text: %s\" %test_statements.values[i])\n",
    "        print(\"Ground Truth: %s\" %y_test.values[i])\n",
    "        print(\"Prediction: %s\" %clf.predict([sample])[0])\n",
    "        no_example -= 1\n",
    "        if no_example == 0:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "The classifier doesn't work very well with negation in the statements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with word embeddings -word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 11.6:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.6.1</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Downloaded the pretrained google news word2vec word embeddings from <br><br> https://drive.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM <br><br> Run the folloing code and explain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gensim module is loading the google word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.6.2</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Generate the embeddding features for test statements. Please remove stopwords (see the train statements example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_features_train = []\n",
    "for sent in train_statements:\n",
    "    sent = [x.lower() for x in sent.split(' ') if x not in stopwords.words('english')]\n",
    "    for i in range(len(sent)):\n",
    "        sent[i] = model[sent[i]] if sent[i] in model.vocab else np.zeros(300)\n",
    "    embedding_features_train.append(np.array(np.mean(sent, axis=0)))\n",
    "\n",
    "embedding_features_train = np.array(embedding_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_features_test = []\n",
    "for sent in test_statements:\n",
    "    sent = [x.lower() for x in sent.split(' ') if x not in stopwords.words('english')]\n",
    "    #TODO\n",
    "\n",
    "embedding_features_test = np.array(embedding_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = embedding_features_train\n",
    "y_train_emb = train_labels\n",
    "\n",
    "X_test_emb = embedding_features_test\n",
    "y_test_emb = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_features_test = []\n",
    "for sent in test_statements:\n",
    "    sent = [x.lower() for x in sent.split(' ') if x not in stopwords.words('english')]\n",
    "    for i in range(len(sent)):\n",
    "        sent[i] = model[sent[i]] if sent[i] in model.vocab else np.zeros(300)\n",
    "    embedding_features_test.append(np.array(np.mean(sent, axis=0)))\n",
    "    \n",
    "embedding_features_test = np.array(embedding_features_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = embedding_features_train\n",
    "y_train_emb = train_labels\n",
    "\n",
    "X_test_emb = embedding_features_test\n",
    "y_test_emb = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.6.3</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Train the naive bayes classifier and evaluate it using confusion metric, accuracy and classification report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_emb = GaussianNB()\n",
    "clf_emb.fit(X_train_emb, y_train_emb)\n",
    "\n",
    "predictions_emb = clf_emb.predict(X_test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_emb = confusion_matrix(y_test_emb,predictions_emb)\n",
    "print(cf_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cf_emb, cmap=\"GnBu\", annot=True, fmt='g');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_emb, predictions_emb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_emb,predictions_emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.6.4</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Find three statements that the classifier fails.<br> Please run the following code snippet to test the classifier. Give a justification for the classifier failure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vector(input_text):\n",
    "    return np.array(np.mean([model[w] for w in input_text.split(' ') if w in model.vocab]\n",
    "                            or [np.zeros(self.dim)], axis=0))\n",
    "\n",
    "\n",
    "# Process user input. Processing continues until the user says goodbye. \n",
    "text = \"\"\n",
    "while text != \"goodbye\":\n",
    "    # Read user input\n",
    "    text = input()\n",
    "    test_features_emb = get_embedding_vector(text) \n",
    "    print('Positive Sentiment' if clf_emb.predict([test_features_emb]) == 1 else 'Negative sentiment')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "i am not sad –– negation\n",
    "\n",
    "product quality is okay okay –– interjection words\n",
    "\n",
    "i hardly like the product –– hardly\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.6.5</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "The error analysis shows that despite a better performance model it is not working well with sentences having a negation. Understand and explain the idea behind the following function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toggle_prediction(prediction, text):\n",
    "    prediction_toggled = [x for x in prediction]\n",
    "    for i, sent in enumerate(text):\n",
    "        sent = sent.lower().split(' ')\n",
    "        if 'not' in sent:\n",
    "            prediction_toggled[i] = 1 - prediction_toggled[i]\n",
    "        else:\n",
    "            for w in sent:\n",
    "                if \"n't\" in w:\n",
    "                    prediction_toggled[i] = 1 - prediction_toggled[i]\n",
    "                    break\n",
    "    return prediction_toggled\n",
    "                \n",
    "predictions_emb_toggled = toggle_prediction(predictions_emb, test_statements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function toggles the prediction if the test statement contains negations.\n",
    "For example words like \"not\", \"haven't\", \"aren't\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.6.6</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Compare the performance with the original classfier prediction results and describe your observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_emb = confusion_matrix(y_test_emb,predictions_emb_toggled)\n",
    "print(cf_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_emb, cmap=\"GnBu\", annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_emb,predictions_emb_toggled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_emb, predictions_emb_toggled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.6.7</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Find three statements that the negation conditioned classifier fails. Please run the following code snippet to test the classifier. Give justifications for the classifier failure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process user input. Processing continues until the user says goodbye. \n",
    "text = \"\"\n",
    "while text != \"goodbye\":\n",
    "    # Read user input\n",
    "    text = input()\n",
    "    test_features_emb = get_embedding_vector(text) \n",
    "    print('Positive Sentiment' if toggle_prediction(clf_emb.predict([test_features_emb]), [text])[0] == 1 else 'Negative sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "1. Does Not Work.\n",
    "2. My car will not accept this cassette.\n",
    "3. It's not what it says it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 11.7:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">11.7.1</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Please go through the following code snippets. Which of them shows where toggling of prediction works and where not. Can you provide a comment based on the code results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_example = 10\n",
    "\n",
    "for i, sample in enumerate(X_test_emb):\n",
    "    if \"n't\" in test_statements.values[i].lower() or \"not\" in test_statements.values[i].lower():\n",
    "        if y_test_emb.values[i] != clf_emb.predict([sample])[0]:\n",
    "            print(\"Text: %s\" %test_statements.values[i])\n",
    "            print(\"Ground Truth: %s\" %y_test_emb.values[i])\n",
    "            print(\"Prediction: %s\" %clf_emb.predict([sample])[0])\n",
    "            print(\"Toggled Prediction: %s\" %int(toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]])[0]))\n",
    "            print()\n",
    "            no_example -= 1\n",
    "            if no_example == 0:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_example = 10\n",
    "\n",
    "for i, sample in enumerate(X_test_emb):\n",
    "    if \"n't\" in test_statements.values[i].lower() or \"not\" in test_statements.values[i].lower():\n",
    "        if y_test_emb.values[i] != int(toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]])[0]):\n",
    "            print(\"Text: %s\" %test_statements.values[i])\n",
    "            print(\"Ground Truth: %s\" %y_test_emb.values[i])\n",
    "            print(\"Prediction: %s\" %clf_emb.predict([sample])[0])\n",
    "            print(\"Toggled Prediction: %s\" %int(toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]])[0]))\n",
    "            print()\n",
    "            no_example -= 1\n",
    "            if no_example == 0:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggling of prediction works\n",
    "\n",
    "no_example = 10\n",
    "\n",
    "\n",
    "for i, sample in enumerate(X_test_emb):\n",
    "    if \"n't\" in test_statements.values[i].lower() or \"not\" in test_statements.values[i].lower():\n",
    "        if y_test_emb.values[i] != clf_emb.predict([sample])[0]:\n",
    "            print(\"Text: %s\" %test_statements.values[i])\n",
    "            print(\"Ground Truth: %s\" %y_test_emb.values[i])\n",
    "            print(\"Prediction: %s\" %clf_emb.predict([sample])[0])\n",
    "            print(\"Toggled Prediction: %s\" %int(toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]])[0]))\n",
    "            print()\n",
    "            no_example -= 1\n",
    "            if no_example == 0:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggling of prediction fails\n",
    "\n",
    "no_example = 10\n",
    "\n",
    "\n",
    "for i, sample in enumerate(X_test_emb):\n",
    "    if \"n't\" in test_statements.values[i].lower() or \"not\" in test_statements.values[i].lower():\n",
    "        if y_test_emb.values[i] != int(toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]])[0]):\n",
    "            print(\"Text: %s\" %test_statements.values[i])\n",
    "            print(\"Ground Truth: %s\" %y_test_emb.values[i])\n",
    "            print(\"Prediction: %s\" %clf_emb.predict([sample])[0])\n",
    "            print(\"Toggled Prediction: %s\" %int(toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]])[0]))\n",
    "            print()\n",
    "            no_example -= 1\n",
    "            if no_example == 0:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "<br>\n",
    "\n",
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 11.1:</i> <br>\n",
    "</div>\n",
    "\n",
    "\n",
    "You have seen how to solve the task by using the Word2Vec embeddings. Now try it yourself using the GloVe embeddings and compare the results. Elaborate your comparison briefly. Comparing only the accuracy is not enough.\n",
    "\n",
    "We recommend using the glove.6B file you know from other exercises.\n",
    "<a href=\"https://nlp.stanford.edu/projects/glove/\">GloVe Website</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

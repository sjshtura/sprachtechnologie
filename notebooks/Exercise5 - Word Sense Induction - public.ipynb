{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/* Style des gesamten Dokuments */\n",
       "#notebook-container {\n",
       "\tfont-family: \"NimbusMonL-ReguObli\";\n",
       "\tfont-size: 120%\n",
       "}\n",
       "\n",
       "/* Style für die Überschrift: Zentriert diese und stellt sie fett dar. */\n",
       ".headline {\n",
       "\ttext-align: center;\n",
       "\tfont-weight: bold;\n",
       "\tfont-size: 185.7%\n",
       "}\n",
       "\n",
       "/* Style für die Aufgabenbeschreibung. Z.B.: \"Übung zum Thema...\" */\n",
       ".description {\n",
       "\ttext-align: center;\n",
       "\tfont-size: 145.7%\n",
       "}\n",
       "\n",
       "/* Hebt das Abgabedatum fett und kursiv hervor */\n",
       "#submission {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "\n",
       "/* Style für das eigentliche Thema. Z.B.: \"Intelligenz\" */\n",
       "#topic {\n",
       "\tfont-style: italic;\n",
       "}\n",
       "\n",
       ".task_description {\n",
       "\tmargin-bottom: 20px;\n",
       "}\n",
       "\n",
       "/* Hebt die Aufgabennummerierung fett hervor. */\n",
       ".task {\n",
       "\tfont-style: normal;\n",
       "\tfont-weight: bold;\n",
       "\tfont-size: 120%;\n",
       "\tborder-bottom: 2px solid black;\n",
       "  background-color: #97CAEF;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 50px;\n",
       "  padding-right: 50px;\n",
       "}\n",
       "\n",
       ".subtask {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #CAFAFE;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 25px;\n",
       "  padding-right: 25px;\n",
       "}\n",
       "\n",
       ".l1 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #14A76C;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".l2 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #FFE400;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".l3 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #FF652F;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".points {\n",
       "\tfont-style: italic;\n",
       "}\n",
       "\n",
       "ol.lower_roman {\n",
       "    list-style-type: lower-roman;\n",
       "}\n",
       "\n",
       "ol.characters {\n",
       "    list-style-type: lower-alpha;\n",
       "}\n",
       "\n",
       "/* Style einer Code-Cell */\n",
       ".CodeMirror-code {\n",
       "\tbackground-color: #ededed\n",
       "}\n",
       "\n",
       "/* Style eines Kommentars im Code ändern. */\n",
       ".cm-s-ipython span.cm-comment {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-atom {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-number {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Keywords ändern */\n",
       ".cm-s-ipython span.cm-keyword {\n",
       "\tcolor: #B000B0\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-def {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style einer Python-Variable ändern */\n",
       ".cm-s-ipython span.cm-variable {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style einer Property ändern */\n",
       ".cm-s-ipython span.cm-property {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Operators ändern */\n",
       ".cm-s-ipython span.cm-operator {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Strings ändern */\n",
       ".cm-s-ipython span.cm-string {\n",
       "\tcolor: brown;\n",
       "}\n",
       "\n",
       "/* Style einer eingebauten Funktion ändern (z.B. \"open\") */\n",
       ".cm-s-ipython span.cm-builtin {\n",
       "\n",
       "}\n",
       "\n",
       "/* Hebt hervor, welche Klammern zueinander passen */\n",
       ".cm-s-ipython .CodeMirror-matchingbracket {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-variable-2 {\n",
       "\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>\" + open(\"style.css\").read() + \"</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"headline\">\n",
    "Language Technology / Sprachtechnologie\n",
    "<br><br>\n",
    "Wintersemester 2019/2020\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"description\">\n",
    "    Übung zum Thema <i id=\"topic\">\"Word Sense Induction\"</i>\n",
    "    <br><br>\n",
    "    Deadline Abgabe: <i #id=\"submission\">Thursday, 21.11.2019 (23:55 Uhr)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Präsenzübung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn  \n",
    "from nltk.book import text2,text3 \n",
    "from nltk.text import Text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.cluster import KMeans  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.1:</i> <br>\n",
    "</div>\n",
    "\n",
    "Stopword: Which of the following statements are true?\n",
    "\n",
    "1. A stopword is a word used to stop a parsing process.\n",
    "2. A stopword is a high-frequency word.\n",
    "3. Punctuation marks like ``.``, ``,``, and ``;`` are stop words.\n",
    "4. \"the, to, and\" are stop words.\n",
    "5. Stopwords have a maximum length of 3.\n",
    "6. NLTK offers a list of English stopwords through: nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.2:</i> <br>\n",
    "</div>\n",
    "\n",
    "Synset: Which of the following statements are true?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A synset is a set of words that are interchangeable in some context without changing the meaning of a sentence in which they are embedded.\n",
    "2. A synset is a set of all bigrams within wordnet.\n",
    "3. Within W ordnet (corpus / lexical ressource) each word corresponds to one or more synsets.\n",
    "4. You may access the synsets of \"dog\" in NLTK by using nltk.corpus.wordnet.synsets('dog')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.3:</i> <br>\n",
    "</div>\n",
    "There are two possible definitions of 'word':\n",
    "\n",
    "* Word is the same as token, for example ‘see’ and ‘saw’ are different words;\n",
    "* Word is the same as lemma, hence ‘things’ and ‘thing’ becomes the same words. \n",
    "<br><br>Which of the following statements are true?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The lemma of “appeared” is“appear”.\n",
    "2. The lemma of a verb is its infinitive.\n",
    "3. Two words having the same lemma are called homonyms.\n",
    "4. Each token has one and only one lemma.\n",
    "5. Each lemma belongs to one and only one word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.4:</i> <br>\n",
    "</div>\n",
    "\n",
    "A stopword list contains high-frequency words like “the”, “to”, “and”, or “also” that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. NLTK provides a predefined list of stopwords: nltk.corpus.stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.4.1</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Find all non-stopwords in carroll-alice.txt of the corpus Gutenberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.4.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Implement a function is_the_same_vocab(text1,text2) that compares two texts. It should return true, if the texts have the same vocabulary after removing the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.5:</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Take a look at the code below for some examples on how to work with WordNet. You can also execute it on your computer to see how the output looks like for a better understanding.\n",
    "\n",
    "Explain the code line by line, what datatype and meaning have the methods used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wn.synsets('motorcar'),\"\\n\")\n",
    "print(wn.synset('car.n.01').lemma_names(),\"\\n\")\n",
    "print(wn.synset('car.n.01').definition(),\"\\n\")\n",
    "print(wn.synset('car.n.01').examples(),\"\\n\")  \n",
    "print(wn.synset('car.n.01').lemmas(),\"\\n\")\n",
    "print(wn.lemma('car.n.01.automobile'),\"\\n\")  \n",
    "print(wn.lemma('car.n.01.automobile').synset(),\"\\n\")\n",
    "print(wn.lemma('car.n.01.automobile').name(),\"\\n\")  \n",
    "print(wn.synsets('car'),\"\\n\")\n",
    "print(wn.lemmas('car'),\"\\n\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.6:</i> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "List all the senses of the word \"like\" that you can think of. Now we want to see how many different meanings the word ”like” has with the help of WordNet, using the same operations we used above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "As you can see above, the words can have different POS tags. Suppose you are only interested in the adjective \"warm\". Write a function that will output definitions and examples of the word \"warm\" under this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.3.</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "Write functions that outputs the hypernyms and hyponyms of the word 'bank'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.4</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Another aspect we can examine is if a word has an antonym (word with opposite meaning). Write a function that searches an antonym for the word \"like\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.7:</i> <br>\n",
    "</div>\n",
    "The polysemy of a word is the number of senses it has. We now want to compute the average polysemy of nouns, verbs, adjectives and adverbs according to WordNet and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.7.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Try to think of an algorithm that caclulates the ploysemy of a POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.7.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Take a look at the code below. Implement it and enhance the main function so that the average polysemy is calculated not only for nouns but also for verbs, adjectives and adverbs (do not implement your algorithm yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgPolysemy():\n",
    "    #TODO, return average polysemy\n",
    "    \n",
    "avgN = avgPolysemy()\n",
    "    #TODO\n",
    "\n",
    "print(\"average polysemy of nouns: \", + avgN)\n",
    "    #TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.7.3</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Now implement your algorithm and calculate the average polysemy for nouns, verbs, adjectives and adverbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.8:</i> <br>\n",
    "</div>\n",
    "\n",
    "A concordance view shows us every occurrence of a given word, together with some context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.8.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Extract context samples of word ‘affection’ in “Sense and Sensibility” (text2) corpus of nltk and word ‘lived’ in “Book of Genesis” (text3) corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.8.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Similar words for the term can be found if words share same context as that of token. \n",
    "\n",
    "For example: monsterous occurred in contexts such as the ___ pictures and a ___ size . What other words appear in a similar range of contexts? We can find out by appending the term similar to the name of the text in question, then inserting the relevant word in parentheses:\n",
    "\n",
    "Find out similar words for affection in “Sense and Sensibility” (text2) corpus of nltk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering - Tf-Idf Vectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.9:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Extract all the context of term affection in the “Sense and Sensibility” (text2) corpus of nltk with a window size of 21 (each context should have 10 words before and after the term). Provide the total number of contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nltk.ConcordanceIndex(text2.tokens, key=lambda s: s.lower())  \n",
    "window_size = 10  \n",
    "affection_contexts = []  \n",
    "for index in c.offsets('affection'):  \n",
    "    if index > 10:  \n",
    "           #TODO\n",
    "print(len(affection_contexts))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Preprocess the contexts by removing punctuation and stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))  \n",
    "affection_contexts_no_stopwords = []  \n",
    "for context in affection_contexts:  \n",
    "    # remove punctuation\n",
    "    \n",
    "    #TODO\n",
    "    \n",
    "    # remove stopwords  \n",
    "    #TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.3.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Generate a tf-idf feature for the preprocessed text using sci-kit learn feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()  \n",
    "X_tf_idf = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.4.</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Apply k means algorithm with k = 3(number of cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 3 # number of clusters  \n",
    "\n",
    "# apply algorithm \n",
    "\n",
    "model_tf_idf = #TODO\n",
    "\n",
    "# training the algorithm\n",
    "#TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.5.</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Print the top-ten terms that represent each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model_tf_idf.cluster_centers_.argsort()[:, ::-1]  # getting indexes of centroids   \n",
    "terms = vectorizer.get_feature_names() # getting feature terms   \n",
    "for i in range(true_k):  \n",
    "    print(\"Cluster %d:\" % i)\n",
    "    \n",
    "    # get top 10 terms\n",
    "    \n",
    "    #TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.6.</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "Assign context to the corresponding cluster predicted by the k-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}  \n",
    "for context in affection_contexts:  \n",
    "    X = vectorizer.transform([context])  # get tf-idf vector for the context  \n",
    "    predicted = model_tf_idf.predict(X)  # predict the cluster  \n",
    "    \n",
    "    # generate clusters\n",
    "    \n",
    "    # TODO    \n",
    "print(clusters)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.10:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is capable of capturing the meaning of a word in a document, semantic and syntactic similarity, relation with other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Import common_text corpus from gensim library and display first 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Train the Word2vec model based on gensim with the follwing parameters:\n",
    "    - Size of the dimension = 100\n",
    "    - Context window size = 5\n",
    "    - Minimum frequency count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.3.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Display the vector for the word \"computer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.4.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Compute the similarity score between the words 'graph' and 'trees'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.5.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Calculate the top 5 most similar word to the word 'graph'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with pretrained google word2vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.11:</i> <br>\n",
    "</div>\n",
    "\n",
    "The Google word2vec embeddings was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the GoogleNews-vectors-negative300.bin.gz embeddings in your current working directory and unzip it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a 1.53 Gigabytes file. You can download it from here:\n",
    "\n",
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.11.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Generate gensim word2vec model using google pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "google_word2vec_model = #TODO # load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.11.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Operation (king – man) + woman = ? can be performed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_word2vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the (Germany – Berlin) + Moscow = ? operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.1:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.1.1.</i> :::5 Homework points:::\n",
    "</div>\n",
    "\n",
    "Try K-means clustering with genism word2vec embeddings features instead of tf-idf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec #get gensim word2vec feature extraction utility  \n",
    "affection_contexts_no_stopwords = [context.split(' ') for context in affection_contexts_no_stopwords]  \n",
    "\n",
    "# Apply word2vec algorithm\n",
    "\n",
    "w2v = #TODO\n",
    "\n",
    "X_w2v = w2v[w2v.wv.vocab]  \n",
    "\n",
    "\n",
    "model_w2v = KMeans(n_clusters=true_k) \n",
    "\n",
    "# Train kmeans algorithm\n",
    "#TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.1.2.</i> :::5 Homework points:::\n",
    "</div>\n",
    "\n",
    "Try K-means clustering with genism word2vec embeddings features instead of tf-idf features. Evaluate the performance of the k-means algorithm using silhouette metric for embedding and tf-idf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score  \n",
    "\n",
    "model_w2v = KMeans(n_clusters=true_k)  \n",
    "model_w2v.fit(X_w2v)  \n",
    "\n",
    "# print Silhouette scores\n",
    "\n",
    "# TODO \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
